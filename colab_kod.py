# -*- coding: utf-8 -*-
"""60624 kod

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ET0D23fq3FvEr9JUs1j3hdpJX85QN30j
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install datasets
!pip install transformers
!pip install transformers[torch] accelerate -U
!pip install seqeval
!pip install transformers datasets fitz nltk
!pip install PyMuPDF

#VERİ SETİNİN OLUŞTURULMASI İÇİN PREPROCESS KODU

from transformers import AutoTokenizer
import datasets
from datasets import Dataset
import pandas as pd
import re
import string
import nltk
from nltk.tokenize import sent_tokenize
import fitz

nltk.download('punkt')

def lowercase(example):
    #Türkçe karakterleri elle lowercase ediyoruz.
    turkce_mapping = {
        'İ': 'i',
        'I': 'ı',
        'Ç': 'ç',
        'Ş': 'ş',
        'Ğ': 'ğ',
        'Ü': 'ü',
        'Ö': 'ö'
    }
    text = example['preprocessed_cumle']
    for ch1, ch2 in turkce_mapping.items():
        text = text.replace(ch1, ch2)
    example['preprocessed_cumle'] = text.lower()  #Geri kalanını lowercase ediyoruz.
    return example

#Bu fonksiyon datasetin preprocessed_cumle sütununu preprocess etmek için kullanılır
def preprocess(example, sutun_adi):
    # \n - gibi karakterleri textten silen kod kısımları.
    example[sutun_adi] = re.sub(r'\n', '', example[sutun_adi])
    example[sutun_adi] = re.sub(r'-', '', example[sutun_adi])
    example[sutun_adi] = re.sub(r'—', '', example[sutun_adi])
    #Datasette bazı noktalama işaretlerinin sonunda boşluk olmadığı için bunların hepsi tek kelime olarak alınabiliyordu. Bunu çözmek için noktalama işaretlerinden sonra boşluk karakteri koyduk.
    example[sutun_adi] = re.sub(r'([—?.,;!])', r'\1 ', example[sutun_adi])
    #Bir önceki işlem dolayısıyla bazı noktalama işaretlerinin sonunda iki adet boşluk karakteri olduğu için bunu teke indirdik.
    example[sutun_adi] = re.sub(r'\s{2,}', ' ', example[sutun_adi])
    return example

def add_preprocessed_column(example):
    #Datasetimizde orijinal text'i kaybetmeden preprocessed_text adlı yeni bir sütun oluşturuyoruz ve orijinal text'i üzerinde işlemler yapmak üzere boraya kopyalıyoruz.
    example['preprocessed_cumle'] = example['text']
    return example

def remove_punctuation(example):
    #Preprocessed_cumle sütunundan noktalama işaretleri ve sembolleri siliyoruz.
    example['preprocessed_cumle'] = re.sub(r'[^\w\s]', '', example['preprocessed_cumle'])
    return example

#Bu fonksiyonda orijinal text sütunundaki metinlere göre tags adlı bir sütun oluşturulmakta ve her bir satırdaki kelimeler teker teker etiketlenmektedir."""
def add_punctuation_tag(example):
    tags = []
    words = example['preprocessed_cumle'].split() #Kelimelere böldük.
    for word in words:
        if word[0].isupper():
            if word[-1] in string.punctuation: #Kelime büyük harfle başlıyorsa bunun son karakterine bakılacak
                if word.endswith(','):         #eğer son karakter bir noktalama ise KELİMENİN HEM BÜYÜK HARFLE BAŞLADIĞI, HEM DE HANGİ NOKTALAMAYLA BİTTİĞİ kaydedilecek.
                    tags.append('UPPER_VIRGUL')
                elif word.endswith('.'):
                    tags.append('UPPER_NOKTA')
                elif word.endswith('?'):
                    tags.append('UPPER_SORU')
                elif word.endswith(':'):
                    tags.append('UPPER_IKINOKTA')
            else:                              #Son karakter noktalama değilse sadece büyük harfle başladığı kaydedilecek.
                tags.append('UPPER')
        elif word.endswith(','):
            tags.append('VIRGUL')
        elif word.endswith('.'):
            tags.append('NOKTA')
        elif word.endswith('?'):
            tags.append('SORU')
        elif word.endswith(':'):
            tags.append('IKINOKTA')
        else:
            tags.append('NONE')
    example['tags'] = tags
    return example

#Bu fonksiyon kullandığımız tokenizera göre preprocessed_cumle sütununu tokenize edecek ve tokenizasyon sonucu elde edilen sonucu yeni bir sütunda saklayacak.
def tokenize_text(example):
    tokenized = tokenizer.tokenize(example['preprocessed_cumle'])
    example['tokenized'] = tokenized
    return example

#AŞAĞIDAKİ 2 FONKSİYON, DATASETTE TOKENİZER'IN MAX KARAKTER SINIRINI AŞACAK SAYIDA KARAKTER İÇEREN SATIRLAR BULUNDUĞU İÇİN
#BU SATIRLARI BÖLMEK ÜZERE KULLANILMIŞTIR. SATIRLARI BÖLERKEN
#YAKLAŞIK BİR KELİME SAYISI BELİRLEYEREK, METİNİ HER SATIRDA BU KADAR KELİME OLACAK ŞEKİLDE FARKLI SATIRLARA TAŞIDIK.

def split_text_into_chunks(text, target_word_count):
    chunks = [] #chunks adında boş bir liste tanımla.
    sentences = sent_tokenize(text, language='turkish') #burada nltk.tokenize modülünün sentence tokenizer fonksiyonunu kullandık ve metinleri cümlelere böldük.
    current_chunk = []
    current_word_count = 0

    for sentence in sentences:
        #Cümlenin mevcut chunk'a eklenmesi durumunda belirlenen kelime sayısının aşılıp aşılmayacağını kontrol eder. Aşılmayacaksa cümleyi chunk'a ekler.
        if current_word_count + len(sentence.split()) <= target_word_count:
            current_chunk.append(sentence)
            current_word_count += len(sentence.split())
        else:
            #Eğer uzunluk aşılacaksa yeni bir chunk oluşturur.
            chunks.append(' '.join(current_chunk))
            current_chunk = [sentence]
            current_word_count = len(sentence.split())

    #Son chunk'ı ekler.
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

def split_rows(example):
    #Verilen datasete chunks adında bir sütun ekler ve her bir satırda "text" sütunundaki metini
    #belirlenen kelime sınırını aşmamak üzere cümlelere ayırıp chunks satırında birkaç cümleden oluşan bir liste halinde tutar
    text = example["text"]
    chunks = split_text_into_chunks(text, target_word_count=50)
    example["chunks"] = chunks
    return example

#Tagleme işlemleri sonucu ortaya çıkan listedeki eleman sayısı, cümledeki kelime sayısına eşittir.
#Ancak tokenizasyonda bazı kelimeler parçalandığı için token sayısı tag sayısından fazla olacaktır.
#Bu fonksiyonda tags listesi genişletilerek etiketler tokenlere göre yeniden düzenlenmiştir.
def align_tags_with_tokens(example):
    padded_tags = [] #Yeni liste
    prev_tag_index = 0 #Tags listesinin eleman sayısı tokenized'dan az olduğu için burada gezmek için bir indekse ihtiyaç duyuyoruz.
    tags_length = len(example['tags'])

    for token in example['tokenized']:
        #Tokenler ## ile başlamıyorsa (yani ek değilse) orijinal tagler listesinde bu kelimenin etiketi olan etiket yeni listeye eklenir.
        if not token.startswith("##"):
            if prev_tag_index < tags_length:
                padded_tags.append(example['tags'][prev_tag_index])
                prev_tag_index += 1
            else:
                padded_tags.append('NONE')
        #Tokenler ## ile başlıyorsa (yani ek ise) öncelikle NONE eklenir. Sonrasında ise kelimenin tagini en sondaki eke
        #kaydırmak için çeşitli işlemler yapılır. Eğer orijinal kelimenin tagi UPPER_noktalama formatında ise
        #UPPER etiketi kelimeyi temsil eden tokenlerin İLKİNDE, noktalama ise kelimeyi temsil eden tokenlerin en SONDAKİNDE bulunacak
        #şekilde swap işlemleri yapılır.
        else:
            padded_tags.append('NONE')
            if len(padded_tags) > 1:
                prev_tag = padded_tags[-2]
                if prev_tag in ['NOKTA', 'VIRGUL', 'SORU', 'IKINOKTA']:
                    padded_tags[-1], padded_tags[-2] = padded_tags[-2], padded_tags[-1]
                elif prev_tag.startswith('UPPER_'):
                    punctuation = prev_tag.split('_')[1]
                    padded_tags[-2] = 'UPPER'
                    padded_tags[-1] = punctuation
                elif prev_tag == 'UPPER':
                    pass

    example['tags'] = padded_tags #Oluşturulan padded_tags listesi datasetteki tags sütunuyla değiştirilir.
    return example

#Modele verilecek veriler integer olması gerektiğinden, tags listesi integer formata dönüştürülür.
def labels_to_integers(example):
    label_map = {
        'UPPER': 1, 'VIRGUL': 2, 'NOKTA': 3, 'IKINOKTA': 4, 'SORU': 5, 'UPPER_VIRGUL': 6, 'UPPER_NOKTA': 7, 'UPPER_IKINOKTA': 8, 'UPPER_SORU': 9,
        'NONE': 0
    }
    example['integer_labels'] = [label_map[label] for label in example['tags']]
    return example

#Verilen pdf'in belirli sayfalarındaki textleri datasete çekmek için yazılan fonksiyon.
def extract_text_from_pdf(pdf_path, start_page, end_page):
    text = ""
    try:
        pdf_document = fitz.open(pdf_path)
        for page_num in range(start_page - 1, end_page):
            page = pdf_document.load_page(page_num)
            text += page.get_text()
    except Exception as e:
        print(f"Error reading PDF: {e}")
    return text

#Kitaplarda, datasette gürültü yaratacak gereksiz verilerin silinmesi için kullanılan fonksiyon.
def clean_text(text):
    #"13.Bölüm" vs. gibi başlıkları kaldırır.
    text = re.sub(r'\b\d+\.\s*Bölüm\b', '', text)
    #”^144" gibi referansları kaldırır.
    text = re.sub(r'”\^\d+', '', text)
    #Yeni satıra geçme karakterini kaldırır.
    text = re.sub(r'\n+', ' ', text)
    #Yeni satır karakteri tarafından ayrılmış kelimeleri birleştirir.
    text = re.sub(r'\s*\n\s*', ' ', text)
    #Arka arkaya gelen space karakterlerini kaldırır.
    text = re.sub(r'\s{2,}', ' ', text)
    return text

#512 tokenden uzun cümleleri tokenizer'ın boyutunu aşacakları için filtreler.
def filter_long_sentences(example):
    tokens = tokenizer.tokenize(example['preprocessed_cumle'])
    return len(tokens) <= 512 and len(tokens) > 0

if __name__ == "__main__":
    pdf_dir = '/content/drive/MyDrive/proje_verileri' #Dataseti beslemek için kullanılan pdf'ler google drive'dan çekilmiştir.
    book_list = [
        ('jack-london-martin-eden.pdf', 7, 341),
        ('sefiller-victor-hugo.pdf', 3, 244),
        ('charles-dickens-buyuk-umutlar.pdf', 2, 195),
        ('don_kisot.pdf', 3, 128),
        ('suc-ve-ceza.pdf', 12, 776),
        ('gormek.pdf', 9, 363),
        ('bulbulu-oldurmek.pdf', 12, 349),
        ('budala.pdf', 20, 1035),
        ('karamazov-kardesler.pdf', 27, 528),
        ('yilanlarin-ocu.pdf', 15, 420),
        ('anna-karenina.pdf', 9, 1504)
    ]

    #tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-tiny-bert-uncased")
    tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-mini-bert-uncased")
    #tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-small-bert-uncased")
    #tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-medium-bert-uncased")
    #tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-base-bert-uncased")

    all_sentences = []

    #Kitaplardan veri çekerek all_sentences'a atılır. Atılmadan önce verilerde temizleme işlemleri uygulanır.
    for file_name, start_page, end_page in book_list:
        pdf_path = f"{pdf_dir}/{file_name}"
        novel_text = extract_text_from_pdf(pdf_path, start_page, end_page)
        novel_text = clean_text(novel_text)
        chunks = split_text_into_chunks(novel_text, target_word_count=25)
        all_sentences.extend(chunks)

    #Veri seti pandas dataframe olarak oluşturulur.
    df = pd.DataFrame({"text": all_sentences})
    new_dataset = Dataset.from_pandas(df)

    #Bu  aşamadan sonra sırasıyla dataseti temizlemek ve son haline getirmek için fonksiyonlar uygulanmıştır.
    #Her bir aşamadan sonra debugging için örnek veriler yazdırdık.

    print("1: PREPROCESSED EKLE")
    new_dataset = new_dataset.map(add_preprocessed_column)
    print("Örnek veri:")
    for i in range(5):
        print(new_dataset[i])

    print("2: PREPROCESS")
    new_dataset = new_dataset.map(lambda example: preprocess(example, 'preprocessed_cumle'))
    print("Örnek veri:")
    for i in range(5):
        print(new_dataset[i])

    print("3: ADD_PUNCTUATION_TAG")
    new_dataset = new_dataset.map(add_punctuation_tag)
    print("Örnek veri:")
    for i in range(5):
        print(new_dataset[i])

    print("4: REMOVE_PUNCTUATION")
    new_dataset = new_dataset.map(remove_punctuation)
    print("Örnek veri:")
    for i in range(5):
        print(new_dataset[i])

    print("5: PREPROCESS (TEXT COLUMN)")
    new_dataset = new_dataset.map(lambda example: preprocess(example, 'text'))
    print("Örnek veri:")
    for i in range(5):
        print(new_dataset[i])

    print("6: LOWERCASE")
    new_dataset = new_dataset.map(lowercase)
    print("Örnek veri:")
    for i in range(5):
        print(new_dataset[i])

    print("7: TOKENIZE_TEXT")
    new_dataset = new_dataset.filter(filter_long_sentences)
    new_dataset = new_dataset.map(tokenize_text)
    print("Örnek veri:")
    for i in range(5):
        print(new_dataset[i])

    print("8: ALIGN TAGS WITH TOKENS")
    new_dataset = new_dataset.map(align_tags_with_tokens)
    print("Örnek veri:")
    for i in range(5):
        print(new_dataset[i])

    print("9: LABELS TO INTEGERS")
    new_dataset = new_dataset.map(labels_to_integers)
    print("Örnek veri:")
    for i in range(5):
        print(new_dataset[i])

    columns_to_remove = ["text", "preprocessed_cumle", "tags"]

    print("10: REMOVE COLUMNS")
    new_dataset = new_dataset.remove_columns(columns_to_remove)
    print("Örnek veri:")
    for i in range(5):
        print(new_dataset[i])

    print("Kalan sütunlar:")
    print(new_dataset.column_names)

    print("Örnek veri:")
    for i in range(5):
        print(new_dataset[i])

#Datasetteki noktalama işaretlerinin sıklığını gösteren kod.

import pandas as pd
from collections import Counter

#'integer_labels' sütununu bir listeye çeviriyoruz.
all_labels = [label for sublist in new_dataset['integer_labels'] for label in sublist]

#Her bir etiketin frekansını sayacak bir counter oluşturuyoruz.
label_counts = Counter(all_labels)

#integer_labels'ı önceki hallerine mapliyoruz.
label_map = {
    0: 'NONE', 1: 'UPPER', 2: 'VIRGUL', 3: 'NOKTA', 4: 'IKINOKTA', 5: 'SORU',
    6: 'UPPER_VIRGUL', 7: 'UPPER_NOKTA', 8: 'UPPER_IKINOKTA', 9: 'UPPER_SORU'
}

#Frekansları daha okunaklı kılmak için dataframe'e dönüştürüyoruz.
label_counts_df = pd.DataFrame.from_dict(label_counts, orient='index', columns=['Frequency'])
label_counts_df.index = label_counts_df.index.map(label_map)

#Sıklığa göre sıralıyor ve yazdırıyoruz.
label_counts_df = label_counts_df.sort_values(by='Frequency', ascending=False)
print(label_counts_df)

#TRAINING KODU

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import Trainer, TrainingArguments, BertForTokenClassification, BertTokenizer
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler
from sklearn.metrics import recall_score, f1_score
import numpy as np
import json
import os
from google.colab import drive

#Class weight kullanan bir FocalLoss sınıfı tanımlanır.
class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        BCE_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)
        pt = torch.exp(-BCE_loss)
        F_loss = (1 - pt) ** self.gamma * BCE_loss

        if self.reduction == 'mean':
            return torch.mean(F_loss)
        elif self.reduction == 'sum':
            return torch.sum(F_loss)
        else:
            return F_loss

#Her bir etiketin ağırlığı sıklıklarına göre deneysel olarak etiketlenmiştir.
class_weights = torch.tensor([1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]).cuda()

#Trainer
class CustomTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.criterion = FocalLoss(alpha=class_weights, gamma=2, reduction='mean')

    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        loss = self.criterion(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

#PunctuationDataset sınıfı
class PunctuationDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-tiny-bert-uncased")
#tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-mini-bert-uncased")
#tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-small-bert-uncased")
#tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-medium-bert-uncased")
#tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-base-bert-uncased")

#Tokenize edilen metni ve integer_labels'ı BERT için encode eden fonksiyon.
def encode_examples(tokenized_texts, labels):
    input_ids = []
    attention_masks = []
    label_masks = []

    for i in range(len(tokenized_texts)):
        encoded_dict = tokenizer.encode_plus(
            " ".join(tokenized_texts[i]),  #Tokenları birleştirip kodlanacak cümleyi oluşturur.
            add_special_tokens=True,  #'[CLS]' ve '[SEP]' özel karakterleri eklenir.
            max_length=512, #padding için
            truncation=True,
            padding='max_length',
            return_attention_mask=True,  #Attention mask oluşturulur.
            return_tensors='pt',  #Pytorch tensorleri döndürülür.
        )
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

        #Padlenen karakterler 0, gerçek değerler 1 olacak şekilde maske oluşturulur.
        label_mask = [-100] + labels[i] + [-100]
        label_mask += [-100] * (512 - len(label_mask))
        label_masks.append(label_mask)

    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0), torch.tensor(label_masks)

input_ids, attention_masks, label_masks = encode_examples(new_dataset['tokenized'], new_dataset['integer_labels'])

#Veri seti train ve validation seti olarak bölünür.
train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, label_masks, test_size=0.1)
train_masks, val_masks, _, _ = train_test_split(attention_masks, label_masks, test_size=0.1)

#Training set için DataLoader oluşturulur.
train_data = PunctuationDataset({'input_ids': train_inputs, 'attention_mask': train_masks}, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)

#Validation set için DataLoader oluşturulur.
val_data = PunctuationDataset({'input_ids': val_inputs, 'attention_mask': val_masks}, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=32)

#Model yüklenir.
model = BertForTokenClassification.from_pretrained(
    "ytu-ce-cosmos/turkish-mini-bert-uncased", #Finetune edilen modele göre değiştirilecek.
    num_labels=10  #10 etiketimiz var.
)

#Training argümanları
training_args = TrainingArguments(
    output_dir='./results',  #output dizini
    num_train_epochs=8,  #epoch sayısı
    per_device_train_batch_size=64,  #training için batch size
    per_device_eval_batch_size=64,  #evaluation için batch size
    warmup_steps=500,  #learning rate zamanlayıcısı için öğrenme adımlarının sayısını belirler
    #// learning rate başlangıçta default olarak = 5e-5
    weight_decay=0.01,  #overfittingi önlemek için kullanılmıştır. her bir adımda
    #ağırlıkların 0.01 oranında azaltılacağı anlamına gelir.
    logging_dir='./logs',  #logları kaydetmek için dizin
    logging_steps=900,
    evaluation_strategy="steps",
    save_strategy="epoch",  #her epoch'ta model kaydedilecektir.
)

#Analiz için metrikleri belirleyen fonksiyon. Hazır kütüphane fonksiyonları kullanılarak recall ve f1 değerleri ölçülmüştür.
def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_labels = [[label for label in label_sequence if label != -100] for label_sequence in labels]
    true_predictions = [
        [label for (label, label_mask) in zip(prediction, label_sequence) if label_mask != -100]
        for prediction, label_sequence in zip(predictions, labels)
    ]

    flat_true_labels = [item for sublist in true_labels for item in sublist]
    flat_true_predictions = [item for sublist in true_predictions for item in sublist]

    recall = recall_score(flat_true_labels, flat_true_predictions, average='macro')
    f1 = f1_score(flat_true_labels, flat_true_predictions, average='macro')

    return {
        "recall": recall,
        "f1": f1
    }

#Trainer başlatılır
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
    compute_metrics=compute_metrics
)

#Model eğitilir.
trainer.train()

#Son model kaydedilir. //dizin eğitilen modele göre elle değiştirilecek.
output_dir = "/content/drive/MyDrive/proje_verileri/model_mini"
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

#Modelin testi yapılır.
eval_results = trainer.evaluate()

#Değerlendirme metrikleri kaydedilir.
metrics_output_path = os.path.join(output_dir, "eval_metrics.json")
with open(metrics_output_path, "w") as f:
    json.dump(eval_results, f)

print(f"Model and metrics saved to {output_dir}")

#Kullanıcıdan rastgele cümle alınarak tahmin yapan kod.

import torch
from transformers import BertTokenizer, BertForTokenClassification

tokenizer = BertTokenizer.from_pretrained("./model_base")
model = BertForTokenClassification.from_pretrained("./model_base")

label_map = {
    'UPPER': 1, 'VIRGUL': 2, 'NOKTA': 3, 'IKINOKTA': 4, 'SORU': 5,
    'UPPER_VIRGUL': 6, 'UPPER_NOKTA': 7, 'UPPER_IKINOKTA': 8, 'UPPER_SORU': 9,
    'NONE': 0
}
reverse_label_map = {v: k for k, v in label_map.items()}

#Input verisini hazırlayıp tahmin almak için fonksiyon.
def predict_punctuation(text):
    inputs = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=512,
        truncation=True,
        padding='max_length',
        return_tensors='pt'
    )
    input_ids = inputs['input_ids']
    attention_mask = inputs['attention_mask']
    model.eval()

    #Gradyan hesaplamalarını devre dışı bırak
    with torch.no_grad():
        #Tahminler alınır.
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits

    #Logitleri tahmin edilen tamsayı etiketlere dönüştürür.
    predictions = torch.argmax(logits, dim=2)

    #Padding ve özel tokenleri kaldırır.
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    predicted_labels = [reverse_label_map[label.item()] for token, label in zip(tokens, predictions[0]) if token not in ['[CLS]', '[SEP]', '[PAD]']]
    tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]', '[PAD]']]

    #Ekleri kelime kökleriyle birleştirir ve labelları buna göre ayarlar.
    final_tokens = []
    final_labels = []
    current_token = ""
    current_label = ""

    for token, label in zip(tokens, predicted_labels):
        if token.startswith("##"):
            current_token += token[2:]
        else:
            if current_token:
                final_tokens.append(current_token)
                final_labels.append(current_label)
            current_token = token
            current_label = label

    if current_token:
        final_tokens.append(current_token)
        final_labels.append(current_label)

    return list(zip(final_tokens, final_labels))

#Örnek kullanım. Buraya rastgele bir cümle girilerek tahminler alınabilir.
input_text = "küçükleri yüz göz kir pas içinde üst başlarını da öyle bulunca bir tokat da grigoriye aşk etti"
predictions = predict_punctuation(input_text)

#Tahminler yazdırılır.
for token, label in predictions:
    print(f"Token: {token}, Label: {label}")